[
  {
    "objectID": "biodiversity-datasets.html",
    "href": "biodiversity-datasets.html",
    "title": "Global Priority Areas",
    "section": "",
    "text": "https://planetarycomputer.microsoft.com/dataset/io-biodiversity\n\noptimized cloud source? Yes (STAC + COG + cloud host)\npublic access? Yes\nLicense: CC-BY"
  },
  {
    "objectID": "biodiversity-datasets.html#io-biodiversity",
    "href": "biodiversity-datasets.html#io-biodiversity",
    "title": "Global Priority Areas",
    "section": "",
    "text": "https://planetarycomputer.microsoft.com/dataset/io-biodiversity\n\noptimized cloud source? Yes (STAC + COG + cloud host)\npublic access? Yes\nLicense: CC-BY"
  },
  {
    "objectID": "biodiversity-datasets.html#iucn-range-maps-redlist-categories",
    "href": "biodiversity-datasets.html#iucn-range-maps-redlist-categories",
    "title": "Global Priority Areas",
    "section": "IUCN Range maps / Redlist categories",
    "text": "IUCN Range maps / Redlist categories\n\nFrom: https://www.iucnredlist.org/resources/spatial-data-download\nMAMMALS.zip ESRI shapefile, 2023-05-02\nREPTILES.zip ESRI shapefile, 2023-05-02\nAMPHIBIANS.zip ESRI shapefile 2023-05-02\nPLANTS.zip ESRI shapefile, 2023-05-02\nREEF_FORMING_CORALS.zip ESRI shapefile, 2023-05-02\nWRASSES_PARROTFISHES.zip ESRI shapefile, 2023-05-02\n\n\nIUCN Aves data comes from Birds of the World, through a separate request process.\n\nbirds/BOTW.gdb ESRI GDB database file, 2023-05-11"
  },
  {
    "objectID": "biodiversity-datasets.html#carbon",
    "href": "biodiversity-datasets.html#carbon",
    "title": "Global Priority Areas",
    "section": "Carbon",
    "text": "Carbon\n\npaper: https://doi.org/10.1038/s41893-021-00803-6\ndownload: https://doi.org/10.5281/zenodo.4091028\nweb: https://irrecoverable.resilienceatlas.org/map\n\nData mirror: cloud-optimized geotif\n\ncarbon/Irrecoverable_Carbon_2010/\n\nIrrecoverable_C_Biomass_2010.tif\nIrrecoverable_C_Soil_2010.tif\nIrrecoverable_C_Total_2010.tif\n\n\nand corresponding same three COG files for:\n\ncarbon/Irrecoverable_Carbon_2018/\ncarbon/Manageable_Carbon_2010/\ncarbon/Manageable_Carbon_2018/\ncarbon/Vulnerable_Carbon_2010/\ncarbon/Vulnerable_Carbon_2018/"
  },
  {
    "objectID": "biodiversity-datasets.html#global-mangrove-watch",
    "href": "biodiversity-datasets.html#global-mangrove-watch",
    "title": "Global Priority Areas",
    "section": "Global Mangrove Watch",
    "text": "Global Mangrove Watch\nhttps://www.globalmangrovewatch.org/\nHas spatial data layers for:\n\nMangrove Habitat Extent\nMangrove Net Change\nMangrove Alerts\nMangrove Restoration\nMangrove Biomass\nMangrove Height\nMangrove Blue Carbon\n\nsee:\n\nhttps://www.mangrovealliance.org/about-us/\nhttps://doi.org/10.5281/zenodo.1469347\n\nMuch more data in Google bucket storage: https://console.cloud.google.com/storage/browser/mangrove_atlas\nS3 products as well, e.g. \n\nhttps://datadownload-production.s3.amazonaws.com/GMW_v3_2015.zip (or in S3 notation:)\ns3://datadownload-production/GMW_v3_2015.zip\n\nlikewise:\n\nGMW_v3_2016.zip\nGMW_v3_2017.zip\nGMW_v3_2019.zip\nGMW_v3_2020.zip\n\n(note 2018 is missing)\nLocalized copies on biodiversity bucket"
  },
  {
    "objectID": "biodiversity-datasets.html#world-protected-areas",
    "href": "biodiversity-datasets.html#world-protected-areas",
    "title": "Global Priority Areas",
    "section": "World Protected Areas",
    "text": "World Protected Areas\nSource site: https://www.protectedplanet.net/en/thematic-areas/wdpa?tab=WDPA license: non-commercial https://www.protectedplanet.net/en/legal human-mediated request required: No\n\nWorld-Protected-Areas-May2023.gdb\n\nmaybe also WDOECMs (https://www.protectedplanet.net/en/thematic-areas/oecms?tab=OECMs)"
  },
  {
    "objectID": "biodiversity-datasets.html#key-biodiversity-areas-kbas",
    "href": "biodiversity-datasets.html#key-biodiversity-areas-kbas",
    "title": "Global Priority Areas",
    "section": "Key Biodiversity Areas (KBAs)",
    "text": "Key Biodiversity Areas (KBAs)"
  },
  {
    "objectID": "biodiversity-datasets.html#ecoregions",
    "href": "biodiversity-datasets.html#ecoregions",
    "title": "Global Priority Areas",
    "section": "EcoRegions",
    "text": "EcoRegions\nPublicly accessible, CC-BY zip:\nhttps://storage.googleapis.com/teow2016/Ecoregions2017.zip\nhttps://developers.google.com/earth-engine/datasets/catalog/RESOLVE_ECOREGIONS_2017\nResolve 2017 EcoRegions data - Web App https://ecoregions2017.appspot.com/ - Earth Engine entry: https://developers.google.com/earth-engine/datasets/catalog/RESOLVE_ECOREGIONS_2017\nLocal snapshot:"
  },
  {
    "objectID": "biodiversity-datasets.html#predicts",
    "href": "biodiversity-datasets.html#predicts",
    "title": "Global Priority Areas",
    "section": "PREDICTS",
    "text": "PREDICTS\n\nref:"
  },
  {
    "objectID": "biodiversity-datasets.html#biotime",
    "href": "biodiversity-datasets.html#biotime",
    "title": "Global Priority Areas",
    "section": "BioTIME",
    "text": "BioTIME"
  },
  {
    "objectID": "biodiversity-datasets.html#lpi",
    "href": "biodiversity-datasets.html#lpi",
    "title": "Global Priority Areas",
    "section": "LPI",
    "text": "LPI"
  },
  {
    "objectID": "biodiversity-datasets.html#gbif",
    "href": "biodiversity-datasets.html#gbif",
    "title": "Global Priority Areas",
    "section": "GBIF",
    "text": "GBIF"
  },
  {
    "objectID": "biodiversity-datasets.html#ebird",
    "href": "biodiversity-datasets.html#ebird",
    "title": "Global Priority Areas",
    "section": "eBird",
    "text": "eBird"
  },
  {
    "objectID": "biodiversity-datasets.html#movebank",
    "href": "biodiversity-datasets.html#movebank",
    "title": "Global Priority Areas",
    "section": "MoveBank",
    "text": "MoveBank\n\nRelevant abiotic environment / climate layers:"
  },
  {
    "objectID": "examples/stac_tiling.html",
    "href": "examples/stac_tiling.html",
    "title": "Stac Tiling",
    "section": "",
    "text": "R\nHere we compute NDVI from sentinel2 over arbitrary data cube:\n\nlibrary(rstac)\nlibrary(gdalcubes)\nlibrary(stars)\n\nLoading required package: abind\n\n\nLoading required package: sf\n\n\nLinking to GEOS 3.11.2, GDAL 3.6.4, PROJ 9.2.0; sf_use_s2() is TRUE\n\nlibrary(tmap)\n\nThe legacy packages maptools, rgdal, and rgeos, underpinning this package\nwill retire shortly. Please refer to R-spatial evolution reports on\nhttps://r-spatial.org/r/2023/05/15/evolution4.html for details.\nThis package is now running under evolution status 0 \n\n\nWe will search the STAC catalog for all satellite images in our desired space & time cube:\n\n## STAC Search over 400 million assets.\nbox &lt;- c(xmin=-122.51006, ymin=37.70801, xmax=-122.36268, ymax=37.80668) \nstart_date &lt;- \"2022-06-01\"\nend_date &lt;- \"2022-08-01\"\n\nitems &lt;- \n  stac(\"https://earth-search.aws.element84.com/v0/\") |&gt;\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox = box,\n              datetime = paste(start_date, end_date, sep=\"/\"),\n              limit = 100) |&gt;\n  post_request() \n\nWe create an image collection of those features that have our desired bands, including the cloud image mask, and filtering out images with over 20% cloud covered (as denoted in their stac metadata)\n\ncol &lt;-\n  stac_image_collection(items$features,\n                        asset_names = c(\"B04\",\"B08\", \"SCL\"),\n                        property_filter = \\(x) {x[[\"eo:cloud_cover\"]] &lt; 20})\n\nWe can now define our cube of interest. Note that this specification is completely independent of the actual data assets – the actual spatial and temporal resolution of the data could be finer or coarser than we request! A given x,y,t pixel in this abstract cube may be covered by multiple images (e.g. overlapping images, or multiple satellite fly-bys in the same week) – and the code will aggregate them by the desired aggregation method (median). Other such x,y,t pixels in the cube may have no data coverage at all – perhaps the only images covering those pixels were removed by the cloud mask. In this case, the pixels could be filled in by the resampling method (such as a spline, nearest-neighbor, or in this case, average of nearby pixels, See gdalwarp. Note that this approach means we can define both the projection and the spatio-temporal resolution of the output completely independently from the input.\n\ncube &lt;- cube_view(srs = \"EPSG:4326\",  \n                  extent = list(t0 = start_date, t1 = end_date,\n                                left = box[1], right = box[3],\n                                top = box[4], bottom = box[2]),\n                  nx = 2400, ny = 2400, dt = \"P1D\",\n                  aggregation = \"median\", resampling = \"average\")\n\nS2.mask &lt;- image_mask(\"SCL\", values=c(3,8,9)) # mask clouds and cloud shadows\n\nvirtual_cube &lt;- raster_cube(col, cube, mask = S2.mask)\n\nObserve that this evaluation is “lazy”, we haven’t yet downloaded a single pixel of satellite imagery. Using our virtual cube, we can reference just the bands of interest and perform arbitrary operations on the those pixels, e.g. to calculate NDVI. This entire calculation happens over network interface range requests, first computing the aggregation and resampling warps. See the gdalcubes paper for details.\n\nndvi &lt;- virtual_cube |&gt;\n  select_bands(c(\"B04\", \"B08\")) |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  reduce_time(c(\"mean(NDVI)\")) |&gt;\n  st_as_stars()\n\ntm_shape(ndvi) + tm_raster(palette = viridisLite::mako(20), n=20)\n\nstars object downsampled to 1000 by 1000 cells. See tm_shape manual (argument raster.downsample)\n\n\n\n\n\n\n\nPython\npystac + stackstac provides a comparable approach to rstac + gdalcubes approach.\n\nimport pystac\nimport stackstac"
  },
  {
    "objectID": "examples/cloud-basics.html",
    "href": "examples/cloud-basics.html",
    "title": "cloud basics",
    "section": "",
    "text": "Sys.setenv(\"AWS_S3_ENDPOINT\"=\"minio.carlboettiger.info\")\nSys.setenv(\"AWS_VIRTUAL_HOSTING\"=\"FALSE\")\n\n\n\n\nlibrary(sf)\nlibrary(stars)\nlibrary(rstac)\nlibrary(gdalcubes)\n\n\nlibrary(dplyr)\nlibrary(spData)\nindia &lt;- spData::world |&gt; filter(name_long==\"India\") \n\nSingle raster:\n\nurl &lt;- \"/vsis3/public-biodiversity/carbon/Irrecoverable_Carbon_2010/Irrecoverable_C_Total_2010.tif\"\n\nBoth stars and terra handle lazy reads well. Stars remains ‘lazy’ even after cropping, while terra evaluates at first call to crop.\n\nr &lt;- stars::read_stars(url)\nr_cropped &lt;- r |&gt; st_crop(india)\nplot(r_cropped)\n\n\n\n\n\n\n\nlibrary(terra)\nr &lt;- terra::rast(url)\nr_cropped &lt;- r |&gt; terra::crop(vect(india))\nplot(r_cropped)\n\n\n\n\n\n\n\ngdalcubes is most powerful in working with large numbers of individual raster assets that are to be combined into a space-time “data cube”. However, it works very nicely even with a single or small collection of rasters as a high-level interface for lazy reads, including cropping, coordinate transforms, and upscaling or downscaling spatial sampling. The following is substantially faster than the alternative methods.\n\nbox &lt;- st_bbox(india)\nview &lt;- cube_view(srs = \"EPSG:4326\",\n               extent = list(t0 = \"2010-01-01\", t1 = \"2010-01-01\",\n                             left = box[1], right = box[3],\n                             top = box[4], bottom = box[2]),\n               nx = 400, ny=400, dt = \"P1Y\")\n\nimg &lt;- gdalcubes::create_image_collection(url, date_time = \"2010-01-01\")\nraster_cube(img, view) |&gt; plot(col = viridisLite::viridis)\n\n\n\n\nLike terra and stars, gdalcubes includes a variety of functions for pixel-based operations across different bands, often with better performance through optimized lazy-evaluation. Otherwise, use stars::st_as_stars() to coerce a raster_cube object to a stars object (e.g. for tmap and other plotting.)\n\n\n\nIn many spatial workflows it desirable to write out intermediate or final products to files that can be rendered for visualization or consumed by other software or workflows. Just as we can read directly from our S3 cloud storage systems, we can also write our outputs back to them. (Note that not all gdal drivers supporting VSI reads support VSI writes.)\n\n\n\n\nSub-setting spatial vector objects works a little differently. In both terra and sf, it is best to specify sub-setting when first opening the vector files (recent versions of terra let us delay this a bit).\n\nwpa_url &lt;- \"/vsis3/biodiversity/World-Protected-Areas-May2023.gdb\"\n\n\n\n\nwpas &lt;- vect(wpa_url, filter = vect(india))\n\nWe can remotely filter polygon data by other columns as well by using postgis query syntax:\n\nnames &lt;- vect(wpa_url, query = \"SELECT DISTINCT NAME FROM WDPA_WDOECM_poly_May2023_all\")\nhead(names$NAME)\n\n[1] \"Hollis Road\"     \"Wairakau\"        \"Waihou Forest\"   \"Wairere Falls\"  \n[5] \"Upland Rd\"       \"Waitioka Stream\"\n\n\n\n\n\nsf syntax is functionally equivalent, but takes the filter as wkt text.\n\nwpas2 &lt;- st_read(wpa_url, wkt = st_as_text(st_geometry(india)))\n\nPlots combining vector and raster layers with tmap\n\nlibrary(tmap)\nr &lt;- raster_cube(img, view) |&gt; st_as_stars()\n\ntm_shape(r) + tm_raster(legend.show = FALSE) +\n  tm_shape(st_as_sf(wpas)) + tm_polygons(alpha=0.5)"
  },
  {
    "objectID": "examples/cloud-basics.html#rasters",
    "href": "examples/cloud-basics.html#rasters",
    "title": "cloud basics",
    "section": "",
    "text": "library(sf)\nlibrary(stars)\nlibrary(rstac)\nlibrary(gdalcubes)\n\n\nlibrary(dplyr)\nlibrary(spData)\nindia &lt;- spData::world |&gt; filter(name_long==\"India\") \n\nSingle raster:\n\nurl &lt;- \"/vsis3/public-biodiversity/carbon/Irrecoverable_Carbon_2010/Irrecoverable_C_Total_2010.tif\"\n\nBoth stars and terra handle lazy reads well. Stars remains ‘lazy’ even after cropping, while terra evaluates at first call to crop.\n\nr &lt;- stars::read_stars(url)\nr_cropped &lt;- r |&gt; st_crop(india)\nplot(r_cropped)\n\n\n\n\n\n\n\nlibrary(terra)\nr &lt;- terra::rast(url)\nr_cropped &lt;- r |&gt; terra::crop(vect(india))\nplot(r_cropped)\n\n\n\n\n\n\n\ngdalcubes is most powerful in working with large numbers of individual raster assets that are to be combined into a space-time “data cube”. However, it works very nicely even with a single or small collection of rasters as a high-level interface for lazy reads, including cropping, coordinate transforms, and upscaling or downscaling spatial sampling. The following is substantially faster than the alternative methods.\n\nbox &lt;- st_bbox(india)\nview &lt;- cube_view(srs = \"EPSG:4326\",\n               extent = list(t0 = \"2010-01-01\", t1 = \"2010-01-01\",\n                             left = box[1], right = box[3],\n                             top = box[4], bottom = box[2]),\n               nx = 400, ny=400, dt = \"P1Y\")\n\nimg &lt;- gdalcubes::create_image_collection(url, date_time = \"2010-01-01\")\nraster_cube(img, view) |&gt; plot(col = viridisLite::viridis)\n\n\n\n\nLike terra and stars, gdalcubes includes a variety of functions for pixel-based operations across different bands, often with better performance through optimized lazy-evaluation. Otherwise, use stars::st_as_stars() to coerce a raster_cube object to a stars object (e.g. for tmap and other plotting.)\n\n\n\nIn many spatial workflows it desirable to write out intermediate or final products to files that can be rendered for visualization or consumed by other software or workflows. Just as we can read directly from our S3 cloud storage systems, we can also write our outputs back to them. (Note that not all gdal drivers supporting VSI reads support VSI writes.)"
  },
  {
    "objectID": "examples/cloud-basics.html#vectors",
    "href": "examples/cloud-basics.html#vectors",
    "title": "cloud basics",
    "section": "",
    "text": "Sub-setting spatial vector objects works a little differently. In both terra and sf, it is best to specify sub-setting when first opening the vector files (recent versions of terra let us delay this a bit).\n\nwpa_url &lt;- \"/vsis3/biodiversity/World-Protected-Areas-May2023.gdb\"\n\n\n\n\nwpas &lt;- vect(wpa_url, filter = vect(india))\n\nWe can remotely filter polygon data by other columns as well by using postgis query syntax:\n\nnames &lt;- vect(wpa_url, query = \"SELECT DISTINCT NAME FROM WDPA_WDOECM_poly_May2023_all\")\nhead(names$NAME)\n\n[1] \"Hollis Road\"     \"Wairakau\"        \"Waihou Forest\"   \"Wairere Falls\"  \n[5] \"Upland Rd\"       \"Waitioka Stream\"\n\n\n\n\n\nsf syntax is functionally equivalent, but takes the filter as wkt text.\n\nwpas2 &lt;- st_read(wpa_url, wkt = st_as_text(st_geometry(india)))\n\nPlots combining vector and raster layers with tmap\n\nlibrary(tmap)\nr &lt;- raster_cube(img, view) |&gt; st_as_stars()\n\ntm_shape(r) + tm_raster(legend.show = FALSE) +\n  tm_shape(st_as_sf(wpas)) + tm_polygons(alpha=0.5)"
  },
  {
    "objectID": "examples/ace.html",
    "href": "examples/ace.html",
    "title": "Optimizing formats for virtual filesystems",
    "section": "",
    "text": "Relevant geospatial data is not always formatted for optimum performance in analysis pipelines. Not all geospatial formats are compatible with virtual filesystem access at all, while others (zipped shapefiles, netcdf, gribs) are usable but less efficient when accessed over network-based interfaces. Meanwhile, many recent formats, such as Cloud Optimized Geotiff, Zarr, or geoparquet excel at this. This enables fast, seamless pipelines without assuming that data is first downloaded in full, which can be both less efficient and add friction to production pipelines that need access to constantly evolving data streams, like satellite imagery, to provide up-to-date indicators."
  },
  {
    "objectID": "examples/ace.html#rasterizing",
    "href": "examples/ace.html#rasterizing",
    "title": "Optimizing formats for virtual filesystems",
    "section": "Rasterizing",
    "text": "Rasterizing\nHere we take a look at data from the California Fish and Wildlife Areas of Conservation Emphasis (ACE). The original data product offers a set of tiled hexes, which is not particularly efficient for some analysis or visualization pipelines. Here we convert to raster versions:\n\nlibrary(stars)\nlibrary(dplyr)\nlibrary(sf)\n\nTerrestrial climate change resilience, copy URL from DSE Biodiversity Catalog\n\n# note, in sf this is not actually a lazy read\ndata &lt;- \"/vsicurl/https://minio.carlboettiger.info/public-biodiversity/ACE_summary/ds2738.gdb\"\nex &lt;- sf::st_read(data)\n\nReading layer `ds2738' from data source \n  `/vsicurl/https://minio.carlboettiger.info/public-biodiversity/ACE_summary/ds2738.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 63982 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -373987.9 ymin: -604495.8 xmax: 540034.3 ymax: 450023.2\nProjected CRS: NAD83 / California Albers\n\nnames(ex)\n\n [1] \"Hex_ID\"          \"CLIM_RANK\"       \"VegRefugiaScore\" \"pct_hex\"        \n [5] \"Eco_Sect\"        \"Eco_Name\"        \"County\"          \"Shape_Length\"   \n [9] \"Shape_Area\"      \"Shape\"          \n\n\nTypically we would specify grid resolution we want for our raster, but in this case we can also allow stars to guess an appropriate scale based on the sizes of the original hexagon tiles. We will rasterize each value column, in this case, the climate resilience rank (on scale 1-5) and the vegetation refugia (continous scale 0-1):\n\nclimate_resilience &lt;-\n  ex |&gt; \n  select(CLIM_RANK) |&gt;\n  stars::st_rasterize()\n\n# optional static plot:\n# plot(climate_resilience, col=viridisLite::turbo(n = 5), nbreaks=6)\n\n\nveg_refugia &lt;- ex |&gt; select(VegRefugiaScore) |&gt; stars::st_rasterize() \n\nWe can now easily add these as layers on an interactive leaflet map: (Trying this with the original hexes is far more resource-intensive in leaflet!) Use the map controls to alter the base map and toggle the layers:\n\nlibrary(tmap)\ntmap_mode(\"view\")\n\ntm_shape(veg_refugia) + \n  tm_raster(palette = viridisLite::mako(9), legend.show = FALSE, alpha=0.8) +   \n  tm_shape(climate_resilience) + \n  tm_raster(palette = viridisLite::turbo(5), legend.show = FALSE, alpha=0.8)\n\n\n\n\n\nIn this"
  },
  {
    "objectID": "examples/stac_search.html",
    "href": "examples/stac_search.html",
    "title": "stac search",
    "section": "",
    "text": "R\nThe R client, rstac, only supports the STAC API, not static catalogs. For now, it can be used in conjunction with external stac catalogs (e.g. Planetary Computer), but cannot search the static biodiversity catalog. The static catalogues are merely JSON files and can be easily parsed with tools like jsonlite.\n\nlibrary(jsonlite)\n\n\n\npython\nThe python client, pystac, provides a rich programmatic methods."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About the Biodiversity Dataset Catalog",
    "section": "",
    "text": "Our Biodiversity Dataset Catalog seeks to make a range of biodiversty data more readily findable, accessible and interoperable (see FAIR principles), while in a manner consistent with data sovereignty and ethics (CARE principles). Data products listed here support access with portable, cloud native, high-performance protocols that avoid the need to download entire data files. When existing data providers already meet these objectives, those sources are simply linked from the catalog. Other sources are mirrored on a local object store based on an open source software platform which supports the widely used S3 API for cloud-native operations (MINIO). Access to mirrored content may require access keys to meet licensing and re-distribution policies of certain data providers. The entire setup, including catalog and object store, can be easily replicated on commodity hardware using only open source tools."
  },
  {
    "objectID": "index.html#account-keys-and-settings",
    "href": "index.html#account-keys-and-settings",
    "title": "About the Biodiversity Dataset Catalog",
    "section": "Account keys and settings",
    "text": "Account keys and settings\nAccess keys are required for data that is mirrored on the DSE cloud object store and for which licensing requirements prohibit re-distribution. Enter your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in ~/.aws/credentials file or as environmental variables. Some objects mirrored to the DSE object store are not subject to licenses preventing redistribution, and are made available on public buckets. These can be accessed with empty key/secret credentials, or by setting AWS_NO_SIGN_REQUEST=FALSE.\nTo let the software know that data is not hosted on AWS but rather, at an independent object store, we must specify the domain of the new location as AWS_S3_ENDPOINT environmental variable, and indicate that our system does not use ‘virtual hosting’ style paths, AWS_VIRTUAL_HOSTING=FALSE. Environmental variables can be set in ~/.bashrc (most platforms) or .Renviron file (for RStudio users)); though for clarity we will show how they can be set directly in python or R. See GDAL VFS documentation for details.\nSome data linked in this catalog is already available from public cloud-based object stores such as Azure, Google Storage, or AWS. Note that the former two require their own virtual filesystem prefixes, /vsiaz/ and /vsigs respectively, as described in the docs."
  },
  {
    "objectID": "index.html#python",
    "href": "index.html#python",
    "title": "About the Biodiversity Dataset Catalog",
    "section": "Python",
    "text": "Python\n\nimport os\nos.environ[\"AWS_NO_SIGN_REQUEST\"]=\"TRUE\" # anonymous access for public buckets\nos.environ[\"AWS_S3_ENDPOINT\"]=\"minio.carlboettiger.info\"\nos.environ[\"AWS_VIRTUAL_HOSTING\"]=\"FALSE\"\n\nWe can then read in raster or vector files using the GDAL virtual filesystem (VFS) URLs given in the catalog (click the button to copy the URL to your clipbard)\n\nimport rioxarray\n\nvfs_url = \"/vsis3/public-biodiversity/carbon/Irrecoverable_Carbon_2018/Irrecoverable_C_Total_2018.tif\"\n\nr = rioxarray.open_rasterio(vfs_url)"
  },
  {
    "objectID": "index.html#r",
    "href": "index.html#r",
    "title": "About the Biodiversity Dataset Catalog",
    "section": "R",
    "text": "R\nThis process is nearly identical using any of the popular spatial frameworks in R, e.g. terra, stars, or sf.\n\nSys.setenv(\"AWS_NO_SIGN_REQUEST\"=\"TRUE\") # set FALSE when password is required\nSys.setenv(\"AWS_S3_ENDPOINT\"=\"minio.carlboettiger.info\")\nSys.setenv(\"AWS_VIRTUAL_HOSTING\"=\"FALSE\")\n\n\nlibrary(terra)\n\nterra 1.7.29\n\nvfs_url = \"/vsis3/public-biodiversity/carbon/Irrecoverable_Carbon_2018/Irrecoverable_C_Total_2018.tif\"\nr = rast(vfs_url)"
  },
  {
    "objectID": "index.html#gdal-and-lazy-evaluation",
    "href": "index.html#gdal-and-lazy-evaluation",
    "title": "About the Biodiversity Dataset Catalog",
    "section": "GDAL and lazy evaluation",
    "text": "GDAL and lazy evaluation\nIt is worth becoming familiar with support for GDAL’s lazy evaluation mechanisms in your preferred client platform. These can allow you to subset, crop, re-sample, and transform your data over the remote network connection, which can improve computational speed and saves available disk and RAM space. Illustrative examples of this in R and python are provided in the example notebooks on this site."
  },
  {
    "objectID": "index.html#categories-of-data-access",
    "href": "index.html#categories-of-data-access",
    "title": "About the Biodiversity Dataset Catalog",
    "section": "Categories of Data Access",
    "text": "Categories of Data Access\nDatasets included in this catalog fall into several different categories of access:\n\nData that is publicly available from a high-bandwidth source in optimized format.\nData that is freely available under permissive licenses, but only from low-bandwidth hosts or in non-optimized formats. Data shared on scientific data repositories such as Zenodo usually fall in this category.\nData that is freely available but under restricted use, typically requiring a request (automated or manual). Such distributors often also have low-bandwidth distribution and formats of category 2.\nData that is shared only privately or under explicit non-disclosure contracts.\nSensitive or sovereign data which can only be stored in approved geographic regions or accessed only from secure systems.\n\nThis catalog seeks to help research teams bridge across these diverse types and needs in biodiversity data while respecting necessary restrictions of each category using distributed and open source tooling."
  }
]