[
  {
    "objectID": "biodiversity-datasets.html",
    "href": "biodiversity-datasets.html",
    "title": "Global Priority Areas",
    "section": "",
    "text": "https://planetarycomputer.microsoft.com/dataset/io-biodiversity\n\noptimized cloud source? Yes (STAC + COG + cloud host)\npublic access? Yes\nLicense: CC-BY"
  },
  {
    "objectID": "biodiversity-datasets.html#io-biodiversity",
    "href": "biodiversity-datasets.html#io-biodiversity",
    "title": "Global Priority Areas",
    "section": "",
    "text": "https://planetarycomputer.microsoft.com/dataset/io-biodiversity\n\noptimized cloud source? Yes (STAC + COG + cloud host)\npublic access? Yes\nLicense: CC-BY"
  },
  {
    "objectID": "biodiversity-datasets.html#iucn-range-maps-redlist-categories",
    "href": "biodiversity-datasets.html#iucn-range-maps-redlist-categories",
    "title": "Global Priority Areas",
    "section": "IUCN Range maps / Redlist categories",
    "text": "IUCN Range maps / Redlist categories\n\nFrom: https://www.iucnredlist.org/resources/spatial-data-download\nMAMMALS.zip ESRI shapefile, 2023-05-02\nREPTILES.zip ESRI shapefile, 2023-05-02\nAMPHIBIANS.zip ESRI shapefile 2023-05-02\nPLANTS.zip ESRI shapefile, 2023-05-02\nREEF_FORMING_CORALS.zip ESRI shapefile, 2023-05-02\nWRASSES_PARROTFISHES.zip ESRI shapefile, 2023-05-02\n\n\nIUCN Aves data comes from Birds of the World, through a separate request process.\n\nbirds/BOTW.gdb ESRI GDB database file, 2023-05-11"
  },
  {
    "objectID": "biodiversity-datasets.html#carbon",
    "href": "biodiversity-datasets.html#carbon",
    "title": "Global Priority Areas",
    "section": "Carbon",
    "text": "Carbon\n\npaper: https://doi.org/10.1038/s41893-021-00803-6\ndownload: https://doi.org/10.5281/zenodo.4091028\nweb: https://irrecoverable.resilienceatlas.org/map\n\nData mirror: cloud-optimized geotif\n\ncarbon/Irrecoverable_Carbon_2010/\n\nIrrecoverable_C_Biomass_2010.tif\nIrrecoverable_C_Soil_2010.tif\nIrrecoverable_C_Total_2010.tif\n\n\nand corresponding same three COG files for:\n\ncarbon/Irrecoverable_Carbon_2018/\ncarbon/Manageable_Carbon_2010/\ncarbon/Manageable_Carbon_2018/\ncarbon/Vulnerable_Carbon_2010/\ncarbon/Vulnerable_Carbon_2018/"
  },
  {
    "objectID": "biodiversity-datasets.html#global-mangrove-watch",
    "href": "biodiversity-datasets.html#global-mangrove-watch",
    "title": "Global Priority Areas",
    "section": "Global Mangrove Watch",
    "text": "Global Mangrove Watch\nhttps://www.globalmangrovewatch.org/\nHas spatial data layers for:\n\nMangrove Habitat Extent\nMangrove Net Change\nMangrove Alerts\nMangrove Restoration\nMangrove Biomass\nMangrove Height\nMangrove Blue Carbon\n\nsee:\n\nhttps://www.mangrovealliance.org/about-us/\nhttps://doi.org/10.5281/zenodo.1469347\n\nMuch more data in Google bucket storage: https://console.cloud.google.com/storage/browser/mangrove_atlas\nS3 products as well, e.g. \n\nhttps://datadownload-production.s3.amazonaws.com/GMW_v3_2015.zip (or in S3 notation:)\ns3://datadownload-production/GMW_v3_2015.zip\n\nlikewise:\n\nGMW_v3_2016.zip\nGMW_v3_2017.zip\nGMW_v3_2019.zip\nGMW_v3_2020.zip\n\n(note 2018 is missing)\nLocalized copies on biodiversity bucket"
  },
  {
    "objectID": "biodiversity-datasets.html#world-protected-areas",
    "href": "biodiversity-datasets.html#world-protected-areas",
    "title": "Global Priority Areas",
    "section": "World Protected Areas",
    "text": "World Protected Areas\nSource site: https://www.protectedplanet.net/en/thematic-areas/wdpa?tab=WDPA license: non-commercial https://www.protectedplanet.net/en/legal human-mediated request required: No\n\nWorld-Protected-Areas-May2023.gdb\n\nmaybe also WDOECMs (https://www.protectedplanet.net/en/thematic-areas/oecms?tab=OECMs)"
  },
  {
    "objectID": "biodiversity-datasets.html#key-biodiversity-areas-kbas",
    "href": "biodiversity-datasets.html#key-biodiversity-areas-kbas",
    "title": "Global Priority Areas",
    "section": "Key Biodiversity Areas (KBAs)",
    "text": "Key Biodiversity Areas (KBAs)"
  },
  {
    "objectID": "biodiversity-datasets.html#ecoregions",
    "href": "biodiversity-datasets.html#ecoregions",
    "title": "Global Priority Areas",
    "section": "EcoRegions",
    "text": "EcoRegions\nPublicly accessible, CC-BY zip:\nhttps://storage.googleapis.com/teow2016/Ecoregions2017.zip\nhttps://developers.google.com/earth-engine/datasets/catalog/RESOLVE_ECOREGIONS_2017\nResolve 2017 EcoRegions data - Web App https://ecoregions2017.appspot.com/ - Earth Engine entry: https://developers.google.com/earth-engine/datasets/catalog/RESOLVE_ECOREGIONS_2017\nLocal snapshot:"
  },
  {
    "objectID": "biodiversity-datasets.html#predicts",
    "href": "biodiversity-datasets.html#predicts",
    "title": "Global Priority Areas",
    "section": "PREDICTS",
    "text": "PREDICTS\n\nref:"
  },
  {
    "objectID": "biodiversity-datasets.html#biotime",
    "href": "biodiversity-datasets.html#biotime",
    "title": "Global Priority Areas",
    "section": "BioTIME",
    "text": "BioTIME"
  },
  {
    "objectID": "biodiversity-datasets.html#lpi",
    "href": "biodiversity-datasets.html#lpi",
    "title": "Global Priority Areas",
    "section": "LPI",
    "text": "LPI"
  },
  {
    "objectID": "biodiversity-datasets.html#gbif",
    "href": "biodiversity-datasets.html#gbif",
    "title": "Global Priority Areas",
    "section": "GBIF",
    "text": "GBIF"
  },
  {
    "objectID": "biodiversity-datasets.html#ebird",
    "href": "biodiversity-datasets.html#ebird",
    "title": "Global Priority Areas",
    "section": "eBird",
    "text": "eBird"
  },
  {
    "objectID": "biodiversity-datasets.html#movebank",
    "href": "biodiversity-datasets.html#movebank",
    "title": "Global Priority Areas",
    "section": "MoveBank",
    "text": "MoveBank\n\nRelevant abiotic environment / climate layers:"
  },
  {
    "objectID": "examples/stac_tiling.html",
    "href": "examples/stac_tiling.html",
    "title": "STAC Tiling",
    "section": "",
    "text": "Describing geospatial assets using STAC metadata, as with the Biodiversity Data Catalog, is not merely a convenient way to improve discovery of existing resources – it can also open the door to substantial performance gains. This is most visible when analyses involve large numbers of individual spatial assets, such as with satellite imagery.\nMost geospatial formats, like geotif, embed spatial metadata such as the bounding box, description of bands, spatial projection and resolution into each individual file. Even though range request methods allow us to read these metadata bytes without downloading the entire asset, making such requests over thousands or hundreds of thousands of assets that comprise modern Earth Observation (EO) data quickly becomes a bottleneck. The metadata available in the STAC catalog for all assets (i.e. tif files) allows us to programmatically identify which assets fall into a particular space-time data cube very efficiently.\nHere we illustrate the use of STAC in programmatically constructing a data cube of satellite imagery assets corresponding to the desired set of space, time, and spectral bands, and excluding assets with insufficient quality flags in metadata (in this case, above a given threshold for cloud cover.) This high-level interface allows us to ignore the nitty gritty of individual satellite image files from different bands, taken at different times, and overlapping in different parts of space, and automatically presents that information to us as a coherent data cube. The approach leverages ‘lazy evaluation’ very effectively, allowing us to avoid unnecessary computation while processing data that may be far too large to fit into computer RAM. This approach works similarly in both R and python.\n\nR\nHere we compute NDVI from Sentinel2 over arbitrary data cube:\n\nlibrary(rstac)\nlibrary(gdalcubes)\nlibrary(stars)\nlibrary(tmap)\n\nWe will search the STAC catalog for all satellite images in our desired space & time cube:\n\n## STAC Search over 400 million assets.\nbox &lt;- c(xmin=-122.51006, ymin=37.70801, xmax=-122.36268, ymax=37.80668) \nstart_date &lt;- \"2022-06-01\"\nend_date &lt;- \"2022-08-01\"\n\nitems &lt;- \n  stac(\"https://earth-search.aws.element84.com/v0/\") |&gt;\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox = box,\n              datetime = paste(start_date, end_date, sep=\"/\"),\n              limit = 100) |&gt;\n  post_request() \n\nWe create an image collection of those features that have our desired bands, including the cloud image mask, and filtering out images with over 20% cloud covered (as denoted in their stac metadata)\n\ncol &lt;-\n  stac_image_collection(items$features,\n                        asset_names = c(\"B04\",\"B08\", \"SCL\"),\n                        property_filter = \\(x) {x[[\"eo:cloud_cover\"]] &lt; 20})\n\nWe can now define our cube of interest. Note that this specification is completely independent of the actual data assets – the actual spatial and temporal resolution of the data could be finer or coarser than we request! A given x,y,t pixel in this abstract cube may be covered by multiple images (e.g. overlapping images, or multiple satellite fly-bys in the same week) – and the code will aggregate them by the desired aggregation method (median). Other such x,y,t pixels in the cube may have no data coverage at all – perhaps the only images covering those pixels were removed by the cloud mask. In this case, the pixels could be filled in by the resampling method (such as a spline, nearest-neighbor, or in this case, average of nearby pixels, See gdalwarp. Note that this approach means we can define both the projection and the spatio-temporal resolution of the output completely independently from the input.\n\ncube &lt;- cube_view(srs = \"EPSG:4326\",  \n                  extent = list(t0 = start_date, t1 = end_date,\n                                left = box[1], right = box[3],\n                                top = box[4], bottom = box[2]),\n                  nx = 2400, ny = 2400, dt = \"P1D\",\n                  aggregation = \"median\", resampling = \"average\")\n\nS2.mask &lt;- image_mask(\"SCL\", values=c(3,8,9)) # mask clouds and cloud shadows\n\nvirtual_cube &lt;- raster_cube(col, cube, mask = S2.mask)\n\nObserve that this evaluation is “lazy”, we haven’t yet downloaded a single pixel of satellite imagery. Using our virtual cube, we can reference just the bands of interest and perform arbitrary operations on the those pixels, e.g. to calculate NDVI. This entire calculation happens over network interface range requests, first computing the aggregation and resampling warps. See the gdalcubes paper for details.\n\nndvi &lt;- virtual_cube |&gt;\n  select_bands(c(\"B04\", \"B08\")) |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  reduce_time(c(\"mean(NDVI)\")) |&gt;\n  st_as_stars()\n\ntm_shape(ndvi) + tm_raster(palette = viridisLite::mako(20), n=20)\n\n\n\n\n\n\nPython\npystac + stackstac provides a comparable approach to rstac + gdalcubes approach.\n\nimport pystac\nimport stackstac"
  },
  {
    "objectID": "examples/cloud-basics.html",
    "href": "examples/cloud-basics.html",
    "title": "cloud basics",
    "section": "",
    "text": "Much scientific computing focuses on the premise that data must be downloaded to a local filesystem (hard-drive) before use. This tutorial focuses on techniques for accessing data directly over a network connection to an object store (such as AWS S3, Google Storage, or Azure blobstore) instead. Object stores are not restricted to commercial cloud providers – today, many research compute centers and academic institutions host their own object stores on their own clusters, such as the NSF-funded OpenStorageNetwork, Jetstream2, using open source software like MINIO or Redhat CEPH. These open source options implement the same application programming interface as AWS S3, making them compatible with software written for this first big commercial and most widely used object store.\nYou do not need an actual cloud host to use these patterns – a user could install a MINIO instance on their own laptop. This can be very useful for testing purposes. Setting up MINIO instance on a personal desktop or local server can be an effective way to share data with a larger team. While this pattern is similar to relational databases such as MySQL or Postgres where data is typically accessed on a central server, it is important to note that object stores are a form of ‘static’ storage. When we make a request against a relational database asking for the sum of a slice of data, that computation is performed by the server, and only the resulting sum is sent back to our local machine. When we make such a request against an object store, the slice is extracted from the larger dataset and sent across the network, and the sum is computed on the local machine (streaming)."
  },
  {
    "objectID": "examples/cloud-basics.html#rasters",
    "href": "examples/cloud-basics.html#rasters",
    "title": "cloud basics",
    "section": "Rasters",
    "text": "Rasters\n\nlibrary(sf)\nlibrary(stars)\nlibrary(rstac)\nlibrary(gdalcubes)\n\n\nlibrary(dplyr)\nlibrary(spData)\nindia &lt;- spData::world |&gt; filter(name_long==\"India\") \n\nSingle raster:\n\nurl &lt;- \"/vsis3/public-biodiversity/carbon/Irrecoverable_Carbon_2010/Irrecoverable_C_Total_2010.tif\"\n\nBoth stars and terra handle lazy reads well. Stars remains ‘lazy’ even after cropping, while terra evaluates at first call to crop.\n\nr &lt;- stars::read_stars(url)\nr_cropped &lt;- r |&gt; st_crop(india)\nplot(r_cropped)\n\n\n\n\n\nterra\n\nlibrary(terra)\nr &lt;- terra::rast(url)\nr_cropped &lt;- r |&gt; terra::crop(vect(india))\nplot(r_cropped)\n\n\n\n\n\n\ngdalcubes\ngdalcubes is most powerful in working with large numbers of individual raster assets that are to be combined into a space-time “data cube”. However, it works very nicely even with a single or small collection of rasters as a high-level interface for lazy reads, including cropping, coordinate transforms, and upscaling or downscaling spatial sampling. This approach is also substantially faster than the alternative methods for remote subsetting and lazy eval analysis. See the entry on stac tiling for more details.\n\nbox &lt;- st_bbox(india)\nview &lt;- cube_view(srs = \"EPSG:4326\",\n               extent = list(t0 = \"2010-01-01\", t1 = \"2010-01-01\",\n                             left = box[1], right = box[3],\n                             top = box[4], bottom = box[2]),\n               nx = 400, ny=400, dt = \"P1Y\")\n\nimg &lt;- gdalcubes::create_image_collection(url, date_time = \"2010-01-01\")\nraster_cube(img, view) |&gt; plot(col = viridisLite::viridis)\n\n\n\n\nLike terra and stars, gdalcubes includes a variety of functions for pixel-based operations across different bands, often with better performance through optimized lazy-evaluation. Otherwise, use stars::st_as_stars() to coerce a raster_cube object to a stars object (e.g. for tmap and other plotting.)\n\n\nWrite operations\nIn many spatial workflows it desirable to write out intermediate or final products to files that can be rendered for visualization or consumed by other software or workflows. Just as we can read directly from our S3 cloud storage systems, we can also write our outputs back to them. See the entry on Publishing Data for details."
  },
  {
    "objectID": "examples/cloud-basics.html#vectors",
    "href": "examples/cloud-basics.html#vectors",
    "title": "cloud basics",
    "section": "Vectors",
    "text": "Vectors\nSub-setting spatial vector objects works a little differently. In both terra and sf, it is best to specify sub-setting when first opening the vector files (recent versions of terra let us delay this a bit).\n\nwpa_url &lt;- \"/vsis3/biodiversity/World-Protected-Areas-May2023.gdb\"\n\n\nterra\n\nwpas &lt;- vect(wpa_url, filter = vect(india))\n\nWe can remotely filter polygon data by other columns as well by using postgis query syntax:\n\nnames &lt;- vect(wpa_url, query = \"SELECT DISTINCT NAME FROM WDPA_WDOECM_poly_May2023_all\")\nhead(names$NAME)\n\n[1] \"Hollis Road\"     \"Wairakau\"        \"Waihou Forest\"   \"Wairere Falls\"  \n[5] \"Upland Rd\"       \"Waitioka Stream\"\n\n\n\n\nsf\nsf syntax is functionally equivalent, but takes the filter as wkt text.\n\nwpas2 &lt;- st_read(wpa_url, wkt = st_as_text(st_geometry(india)))\n\nPlots combining vector and raster layers with tmap\n\nlibrary(tmap)\nr &lt;- raster_cube(img, view) |&gt; st_as_stars()\n\ntm_shape(r) + tm_raster(legend.show = FALSE) +\n  tm_shape(st_as_sf(wpas)) + tm_polygons(alpha=0.5)"
  },
  {
    "objectID": "examples/publish.html",
    "href": "examples/publish.html",
    "title": "Creating new catalog entries",
    "section": "",
    "text": "Relevant geospatial data is not always formatted for optimum performance in analysis pipelines. Not all geospatial formats are compatible with virtual filesystem access at all, while others (zipped shapefiles, netcdf, gribs) are usable but less efficient when accessed over network-based interfaces. Meanwhile, many recent formats, such as Cloud Optimized Geotiff, Zarr, or geoparquet excel at this. This enables fast, seamless pipelines without assuming that data is first downloaded in full, which can be both less efficient and add friction to production pipelines that need access to constantly evolving data streams, like satellite imagery, to provide up-to-date indicators."
  },
  {
    "objectID": "examples/publish.html#example-rasterizing-large-vectors",
    "href": "examples/publish.html#example-rasterizing-large-vectors",
    "title": "Creating new catalog entries",
    "section": "Example: Rasterizing large vectors",
    "text": "Example: Rasterizing large vectors\nHere we take a look at data from the California Fish and Wildlife Areas of Conservation Emphasis (ACE). The original data product offers a set of tiled hexes, which is not particularly efficient for some analysis or visualization pipelines. Here we convert to raster versions:\n\nlibrary(stars)\nlibrary(dplyr)\nlibrary(sf)\n\nTerrestrial climate change resilience, copy URL from DSE Biodiversity Catalog\n\n# note, in sf this is not actually a lazy read\ndata &lt;- \"/vsicurl/https://minio.carlboettiger.info/public-biodiversity/ACE_summary/ds2738.gdb\"\nex &lt;- sf::st_read(data)\n\nReading layer `ds2738' from data source \n  `/vsicurl/https://minio.carlboettiger.info/public-biodiversity/ACE_summary/ds2738.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 63982 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -373987.9 ymin: -604495.8 xmax: 540034.3 ymax: 450023.2\nProjected CRS: NAD83 / California Albers\n\nnames(ex)\n\n [1] \"Hex_ID\"          \"CLIM_RANK\"       \"VegRefugiaScore\" \"pct_hex\"        \n [5] \"Eco_Sect\"        \"Eco_Name\"        \"County\"          \"Shape_Length\"   \n [9] \"Shape_Area\"      \"Shape\"          \n\n\nTypically we would specify grid resolution we want for our raster, but in this case we can also allow stars to guess an appropriate scale based on the sizes of the original hexagon tiles. We will rasterize each value column, in this case, the climate resilience rank (on scale 1-5) and the vegetation refugia (continous scale 0-1):\n\nclimate_resilience &lt;-\n  ex |&gt; \n  select(CLIM_RANK) |&gt;\n  stars::st_rasterize()\n\n# optional static plot:\n# plot(climate_resilience, col=viridisLite::turbo(n = 5), nbreaks=6)\n\n\nveg_refugia &lt;- ex |&gt; select(VegRefugiaScore) |&gt; stars::st_rasterize() \n\nWe can now easily add these as layers on an interactive leaflet map: (Trying this with the original hexes is far more resource-intensive in leaflet!) Use the map controls to alter the base map and toggle the layers:\n\nlibrary(tmap)\ntmap_mode(\"view\")\n\ntm_shape(veg_refugia) + \n  tm_raster(palette = viridisLite::mako(9), legend.show = FALSE, alpha=0.8) +   \n  tm_shape(climate_resilience) + \n  tm_raster(palette = viridisLite::turbo(5), legend.show = FALSE, alpha=0.8)"
  },
  {
    "objectID": "examples/publish.html#writing-derived-layers",
    "href": "examples/publish.html#writing-derived-layers",
    "title": "Creating new catalog entries",
    "section": "Writing derived layers",
    "text": "Writing derived layers\nWe can also write derived layers back to our cloud storage system where they can be used in other workflows. Note that not all formats that support VSI reads also support VSI writes (see st_drivers(\"all\")). VSI writes to tif files require an additional environmental variable:\n\nSys.setenv(\"CPL_VSIL_USE_TEMP_FILE_FOR_RANDOM_WRITE\"=\"YES\") \n\nWe can then write directly to bucket storage (assuming we have configured AWS tokens which were granted write privileges)\n\nclimate_resilience |&gt; \n  write_stars(\"/vsis3/public-biodiversity/ace_summary/climate-resilience.tif\")\n\nIn certain cases it may be necessary to write first to local storage and then copy data over to the bucket system. For instance, gdalcubes can stream a given virtual cube into a collections of many constituent COG assets, including ‘overview’ files, but at present this only works with a local directory. A S3 client like aws-cli or minio R package can be used for bulk copies between local directories and the object store.\n\nlibrary(minio) # install_github(\"cboettig/minio\")\nmc(\"cp -r local_dir minio/bucket-name/path\")"
  },
  {
    "objectID": "examples/publish.html#creating-new-stac-catalog-entries",
    "href": "examples/publish.html#creating-new-stac-catalog-entries",
    "title": "Creating new catalog entries",
    "section": "Creating new STAC Catalog Entries",
    "text": "Creating new STAC Catalog Entries\nIn addition to writing out a derived layer, we may want to create a metadata entry which will add the resulting product to the data. We can also write new entries for data products which can already be found on other portals. A stac catalog entry is a simple JSON file which conforms to the STAC spec, and can be generated programmatically or merely written out manually. See the JSON files in the DSE Biodiversity Catalog on GitHub for examples. Simply commit new JSON files to the GitHub repository and link them from the top-level catalog.json or appropriate sub-catalog to add them.\n(Details / embedded examples to come)"
  },
  {
    "objectID": "examples/stac_search.html",
    "href": "examples/stac_search.html",
    "title": "stac search",
    "section": "",
    "text": "R\nThe R client, rstac, only supports the STAC API, not static catalogs. For now, it can be used in conjunction with external stac catalogs (e.g. Planetary Computer), but cannot search the static biodiversity catalog. The static catalogues are merely JSON files and can be easily parsed with tools like jsonlite.\n\nlibrary(jsonlite)\n\n\n\npython\nThe python client, pystac, provides a rich programmatic methods."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About the Biodiversity Dataset Catalog",
    "section": "",
    "text": "Our Biodiversity Dataset Catalog seeks to make a range of biodiversty data more readily findable, accessible and interoperable (see FAIR principles), while in a manner consistent with data sovereignty and ethics (CARE principles). Data products listed here support access with portable, cloud native, high-performance protocols that avoid the need to download entire data files. When existing data providers already meet these objectives, those sources are simply linked from the catalog. Other sources are mirrored on a local object store based on an open source software platform which supports the widely used S3 API for cloud-native operations (MINIO). Access to mirrored content may require access keys to meet licensing and re-distribution policies of certain data providers. The entire setup, including catalog and object store, can be easily replicated on commodity hardware using only open source tools."
  },
  {
    "objectID": "index.html#account-keys-and-settings",
    "href": "index.html#account-keys-and-settings",
    "title": "About the Biodiversity Dataset Catalog",
    "section": "Account keys and settings",
    "text": "Account keys and settings\nAccess keys are required for data that is mirrored on the DSE cloud object store and for which licensing requirements prohibit re-distribution. Enter your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in ~/.aws/credentials file or as environmental variables. Some objects mirrored to the DSE object store are not subject to licenses preventing redistribution, and are made available on public buckets. These can be accessed with empty key/secret credentials, or by setting AWS_NO_SIGN_REQUEST=FALSE.\nTo let the software know that data is not hosted on AWS but rather, at an independent object store, we must specify the domain of the new location as AWS_S3_ENDPOINT environmental variable, and indicate that our system does not use ‘virtual hosting’ style paths, AWS_VIRTUAL_HOSTING=FALSE. Environmental variables can be set in ~/.bashrc (most platforms) or .Renviron file (for RStudio users)); though for clarity we will show how they can be set directly in python or R. See GDAL VFS documentation for details.\nSome data linked in this catalog is already available from public cloud-based object stores such as Azure, Google Storage, or AWS. Note that the former two require their own virtual filesystem prefixes, /vsiaz/ and /vsigs respectively, as described in the docs."
  },
  {
    "objectID": "index.html#python",
    "href": "index.html#python",
    "title": "About the Biodiversity Dataset Catalog",
    "section": "Python",
    "text": "Python\n\nimport os\nos.environ[\"AWS_NO_SIGN_REQUEST\"]=\"TRUE\" # anonymous access for public buckets\nos.environ[\"AWS_S3_ENDPOINT\"]=\"minio.carlboettiger.info\"\nos.environ[\"AWS_VIRTUAL_HOSTING\"]=\"FALSE\"\n\nWe can then read in raster or vector files using the GDAL virtual filesystem (VFS) URLs given in the catalog (click the button to copy the URL to your clipbard)\n\nimport rioxarray\n\nvfs_url = \"/vsis3/public-biodiversity/carbon/Irrecoverable_Carbon_2018/Irrecoverable_C_Total_2018.tif\"\n\nr = rioxarray.open_rasterio(vfs_url)"
  },
  {
    "objectID": "index.html#r",
    "href": "index.html#r",
    "title": "About the Biodiversity Dataset Catalog",
    "section": "R",
    "text": "R\nThis process is nearly identical using any of the popular spatial frameworks in R, e.g. terra, stars, or sf.\n\nSys.setenv(\"AWS_NO_SIGN_REQUEST\"=\"TRUE\") # set FALSE when password is required\nSys.setenv(\"AWS_S3_ENDPOINT\"=\"minio.carlboettiger.info\")\nSys.setenv(\"AWS_VIRTUAL_HOSTING\"=\"FALSE\")\n\n\nlibrary(terra)\n\nterra 1.7.29\n\nvfs_url = \"/vsis3/public-biodiversity/carbon/Irrecoverable_Carbon_2018/Irrecoverable_C_Total_2018.tif\"\nr = rast(vfs_url)"
  },
  {
    "objectID": "index.html#gdal-and-lazy-evaluation",
    "href": "index.html#gdal-and-lazy-evaluation",
    "title": "About the Biodiversity Dataset Catalog",
    "section": "GDAL and lazy evaluation",
    "text": "GDAL and lazy evaluation\nIt is worth becoming familiar with support for GDAL’s lazy evaluation mechanisms in your preferred client platform. These can allow you to subset, crop, re-sample, and transform your data over the remote network connection, which can improve computational speed and saves available disk and RAM space. Illustrative examples of this in R and python are provided in the example notebooks on this site."
  },
  {
    "objectID": "index.html#categories-of-data-access",
    "href": "index.html#categories-of-data-access",
    "title": "About the Biodiversity Dataset Catalog",
    "section": "Categories of Data Access",
    "text": "Categories of Data Access\nDatasets included in this catalog fall into several different categories of access:\n\nData that is publicly available from a high-bandwidth source in optimized format.\nData that is freely available under permissive licenses, but only from low-bandwidth hosts or in non-optimized formats. Data shared on scientific data repositories such as Zenodo usually fall in this category.\nData that is freely available but under restricted use, typically requiring a request (automated or manual). Such distributors often also have low-bandwidth distribution and formats of category 2.\nData that is shared only privately or under explicit non-disclosure contracts.\nSensitive or sovereign data which can only be stored in approved geographic regions or accessed only from secure systems.\n\nThis catalog seeks to help research teams bridge across these diverse types and needs in biodiversity data while respecting necessary restrictions of each category using distributed and open source tooling."
  }
]