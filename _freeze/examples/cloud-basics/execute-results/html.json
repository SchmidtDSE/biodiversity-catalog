{
  "hash": "a7fc7eb9f9d768475393a249445d0d9b",
  "result": {
    "markdown": "---\ntitle: \"cloud basics\"\nformat: html\n---\n\n\n\n\n# R \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.setenv(\"AWS_S3_ENDPOINT\"=\"minio.carlboettiger.info\")\nSys.setenv(\"AWS_VIRTUAL_HOSTING\"=\"FALSE\")\n```\n:::\n\n\n## Rasters\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\nlibrary(stars)\nlibrary(rstac)\nlibrary(gdalcubes)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(spData)\nindia <- spData::world |> filter(name_long==\"India\") \n```\n:::\n\n\nSingle raster:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"/vsis3/public-biodiversity/carbon/Irrecoverable_Carbon_2010/Irrecoverable_C_Total_2010.tif\"\n```\n:::\n\n\n\nBoth `stars` and `terra` handle lazy reads well.  Stars remains 'lazy' even after cropping, while terra evaluates at first call to `crop`.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nr <- stars::read_stars(url)\nr_cropped <- r |> st_crop(india)\nplot(r_cropped)\n```\n\n::: {.cell-output-display}\n![](cloud-basics_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n### terra\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(terra)\nr <- terra::rast(url)\nr_cropped <- r |> terra::crop(vect(india))\nplot(r_cropped)\n```\n\n::: {.cell-output-display}\n![](cloud-basics_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n### gdalcubes\n\n`gdalcubes` is most powerful in working with large numbers of individual raster assets that are to be combined into a space-time \"data cube\".  However, it works very nicely even with a single or small collection of rasters as a high-level interface for lazy reads, including cropping, coordinate transforms, and upscaling or downscaling spatial sampling.  The following is substantially faster than the alternative methods.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbox <- st_bbox(india)\nview <- cube_view(srs = \"EPSG:4326\",\n               extent = list(t0 = \"2010-01-01\", t1 = \"2010-01-01\",\n                             left = box[1], right = box[3],\n                             top = box[4], bottom = box[2]),\n               nx = 400, ny=400, dt = \"P1Y\")\n\nimg <- gdalcubes::create_image_collection(url, date_time = \"2010-01-01\")\nraster_cube(img, view) |> plot(col = viridisLite::viridis)\n```\n\n::: {.cell-output-display}\n![](cloud-basics_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nLike `terra` and `stars`, gdalcubes includes a variety of functions for pixel-based operations across different bands, often with better performance through optimized lazy-evaluation.  Otherwise, use `stars::st_as_stars()` to coerce a `raster_cube` object to  a stars object (e.g. for `tmap` and other plotting.)\n\n### Write operations\n\nIn many spatial workflows it desirable to write out intermediate or final products to files that can be rendered for visualization or consumed by other software or workflows.  Just as we can read directly from our S3 cloud storage systems, we can also write our outputs back to them.  (Note that not all gdal drivers supporting VSI reads support VSI writes.)\n\n## Vectors\n\nSub-setting spatial vector objects works a little differently.  In both `terra` and `sf`, it is best to specify sub-setting when first opening the vector files (recent versions of `terra` let us delay this a bit).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwpa_url <- \"/vsis3/biodiversity/World-Protected-Areas-May2023.gdb\"\n```\n:::\n\n\n### terra\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwpas <- vect(wpa_url, filter = vect(india))\n```\n:::\n\n\nWe can remotely filter polygon data by other columns as well by using postgis query syntax:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames <- vect(wpa_url, query = \"SELECT DISTINCT NAME FROM WDPA_WDOECM_poly_May2023_all\")\nhead(names$NAME)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Hollis Road\"     \"Wairakau\"        \"Waihou Forest\"   \"Wairere Falls\"  \n[5] \"Upland Rd\"       \"Waitioka Stream\"\n```\n:::\n:::\n\n\n### sf\n\nsf syntax is functionally equivalent, but takes the filter as `wkt` text. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nwpas2 <- st_read(wpa_url, wkt = st_as_text(st_geometry(india)))\n```\n:::\n\n\nPlots combining vector and raster layers with `tmap`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tmap)\nr <- raster_cube(img, view) |> st_as_stars()\n\ntm_shape(r) + tm_raster(legend.show = FALSE) +\n  tm_shape(st_as_sf(wpas)) + tm_polygons(alpha=0.5)\n```\n\n::: {.cell-output-display}\n![](cloud-basics_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n# Python\n\n\n(Coming soon)",
    "supporting": [
      "cloud-basics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}